# Lesson 12 — Reinforcement Learning (RL)

## Mục tiêu bài học

* Hiểu khung Markov Decision Process (MDP) và các thành phần chính.
* Nắm được khái niệm Return, Discount factor, Value function (V), Action-value (Q).
* Hiểu Bellman equations, Dynamic Programming (Value/Policy Iteration).
* Biết thuật toán Q-Learning (off-policy) và nguyên tắc exploration vs exploitation.
* Thực hành triển khai RL cơ bản (Q-table) và thử nghiệm môi trường OpenAI Gym (FrozenLake, CartPole, Blackjack).

## Yêu cầu trước buổi học

* Python cơ bản, NumPy.
* Hiểu khái quát về tối ưu hoá, xác suất và thuật toán quy hoạch động cơ bản.

## Tóm tắt nội dung (ngắn gọn)

* **MDP**: một bộ 5-tuple $(\mathcal S,\mathcal A, P, R, \gamma)$ với transition probability $P(s'|s,a)$ và reward function $R(s,a)$.

* **Return (truncated/infinite)**:

  $G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$,

  với discount factor $0\le\gamma<1$.

* **Value functions (tóm tắt)**:

  * State-value: $V^{\pi}(s)=\mathbb{E}_{\pi}[G_t\mid S_t=s]$.
  * Action-value: $Q^{\pi}(s,a)=\mathbb{E}_{\pi}[G_t\mid S_t=s,A_t=a]$.

* **Bellman expectation equation (tóm tắt)**:

  $V^{\pi}(s)=\sum_a\pi(a|s)\sum_{s',r} P(s',r|s,a)\big[ r + \gamma V^{\pi}(s')\big]$.

* **Bellman optimality (tóm tắt)**:

  $V^*(s)=\max_a \sum_{s',r} P(s',r|s,a)\big[ r + \gamma V^*(s')\big]$.

* **Q-Learning (cập nhật)**:

  $Q(s,a) \leftarrow Q(s,a) + \alpha\big[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \big]$,

  trong đó $\alpha$ là learning rate.

* **Exploration vs Exploitation (ví dụ)**: epsilon-greedy — với xác suất $\epsilon$ chọn action ngẫu nhiên, còn lại chọn $\arg\max_a Q(s,a)$.

* **Các phương pháp chính**: Dynamic Programming (Value/Policy Iteration), Monte Carlo, Temporal-Difference (Q-Learning, SARSA), Policy Gradient, và Deep RL (DQN, Policy gradient / Actor-Critic).

## Lưu ý thực hành (tools & môi trường)

* Environment: **OpenAI Gym / Gymnasium** — ví dụ `FrozenLake-v1`, `CartPole-v1`, `Blackjack-v1`.
* Thư viện RL tiện ích: `stable-baselines3`, `rl-baselines3-zoo` (cho DQN, PPO, A2C), nhưng bài cơ bản dùng Q-table.
* Lập trình: lưu Q-table dưới dạng dictionary hoặc mảng NumPy; lưu checkpoint, plot reward theo episode.
* Hyperparameters quan trọng: learning rate $\alpha$, discount $\gamma$, epsilon schedule (decay), số episodes.
* Khi dùng DQN: sử dụng replay buffer, target network, chuẩn hoá input, clip reward nếu cần.

## Mẹo giảng dạy & demo

* Bắt đầu bằng ví dụ trực quan: **FrozenLake** (small grid) để minh họa Q-table học dần chính sách.
* Dùng **Blackjack** để minh họa MDP có partial observation / decision under uncertainty.
* CartPole để minh hoạ khác: khác biệt giữa giải pháp tabular và function-approx.
* Minh hoạ epsilon decay: cho học sinh thấy trade-off giữa khám phá và hội tụ.
* So sánh Q-Learning (off-policy) và SARSA (on-policy) bằng cùng một môi trường.

## Bài tập / Homework

1. **Q-learning trên FrozenLake (8x8 hoặc 4x4)**

   * Cài đặt Q-table, chạy ít nhất 5k episodes, vẽ learning curve (reward trung bình theo window).
   * Thử nghiệm với ít nhất 2 chiến lược epsilon decay và so sánh kết quả.
2. **CartPole với DQN (tuỳ chọn nâng cao)**

   * Sử dụng thư viện `stable-baselines3` hoặc tự triển khai DQN đơn giản; report số episodes cần đạt score threshold.
3. **Mô hình hoá Blackjack dưới dạng MDP (lý thuyết)**

   * Viết MDP tuple, mô tả state, action space, reward, và đề xuất chính sách cơ bản.

## Tài liệu tham khảo & Links (tài liệu gốc)

* **Slides buổi học (gốc, file bạn cung cấp):** `slides/12-rl/slide-12-rl.pdf`.
* **D2L (Dive into Deep Learning) — RL chapter:** [https://d2l.ai/chapter\_reinforcement-learning/index.html](https://d2l.ai/chapter_reinforcement-learning/index.html).
* **ArXiv / Reviews mentioned trong slide:** (ví dụ) *Reinforcement Learning: An Overview* (link trong slide).
* **OpenAI Gym / Gymnasium docs** (tài liệu API môi trường).
* **stable-baselines3** — tài liệu & examples cho DQN, PPO, A2C.

